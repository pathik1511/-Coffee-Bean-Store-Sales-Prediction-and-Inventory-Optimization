# -*- coding: utf-8 -*-
"""Copy of AI-project-master.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JiBcuoACpFW14XQTiPf__SgiUt1Z8G8I
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense, Dropout
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import IPython
import IPython.display

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Coffee_Stores_Data.csv")

df.head()

df.shape

"""## Section 1: Data Visualization"""

df['StoreID'].unique()

len(df['StoreID'].unique())

df['PLU'].unique()

df['BusinessDate'].min()

df['CategoryLvl1Desc'].unique()

df['CategoryLvl2Desc'].unique()

df_static = df[['PLU','Description','ItemType','CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc']].drop_duplicates()

df_static.head()

df_dynamic = df.drop(['Description','ItemType','CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc','GroupID'],axis=1)
df_dynamic['BusinessDate'] = df_dynamic['BusinessDate'].apply(lambda x:int("".join(x.split("-")[0:2])))

#Top Sales Stores
df_store_SQ = pd.DataFrame(df_dynamic.groupby(['StoreID'])['SoldQuantity'].sum()).sort_values(by=['SoldQuantity'],ascending=False)

sns.histplot(df_store_SQ.SoldQuantity)

#Top Sales inv
df_inv_SQ = pd.DataFrame(df_dynamic.groupby(['PLU'])['SoldQuantity'].sum()).sort_values(by=['SoldQuantity'],ascending=False).reset_index()

sns.histplot(df_inv_SQ.SoldQuantity)

plt.figure(figsize=(15,5))
ax = sns.barplot(x="PLU", y="SoldQuantity", data=df_inv_SQ)

fig, ax = plt.subplots(figsize=(8,6))

df_temp = df_dynamic[['BusinessDate','ReceivedQuantity', 'SoldQuantity',
       'EndQuantity', 'StockedOut', 'MissedSales']]

corrMatrix = df_temp.corr()
sns.heatmap(corrMatrix, annot=True)
plt.show()

# After the month of Feb-2020, the sales declined rapidly due to Covid-19
df_temp = pd.DataFrame(df_dynamic.groupby(['BusinessDate'])['SoldQuantity'].sum()).sort_values(by=['BusinessDate'],ascending=True).reset_index()

plt.figure(figsize=(15,6))
plt.xlabel("Year-Month")
plt.ylabel("No of Sales")
ax = plt.plot(df_temp['BusinessDate'],df_temp['SoldQuantity'],)

## April-2020 is the worst affected month in terms of Sales Quantity
df_temp = df_dynamic[df_dynamic['StoreID']==18]
df_temp = pd.DataFrame(df_temp.groupby(['BusinessDate','PLU'])['SoldQuantity'].sum()).sort_values(by=['BusinessDate'],ascending=True).reset_index()

plt.figure(figsize=(15,6))
sns.set_style('whitegrid')
sns.boxplot(x='BusinessDate',y="SoldQuantity",data=df_temp)

df_temp = pd.DataFrame(df_dynamic.groupby(['BusinessDate','PLU'])['ReceivedQuantity'].sum()).sort_values(by=['BusinessDate'],ascending=True).reset_index()

plt.figure(figsize=(15,6))
sns.set_style('whitegrid')
sns.boxplot(x='BusinessDate',y="ReceivedQuantity",data=df_temp)

df_temp = pd.DataFrame(df_dynamic.groupby(['BusinessDate','PLU'])['EndQuantity'].sum()).sort_values(by=['BusinessDate'],ascending=True).reset_index()

plt.figure(figsize=(15,6))
sns.set_style('whitegrid')
sns.boxplot(x='BusinessDate',y="EndQuantity",data=df_temp)

df_temp = pd.DataFrame(df_dynamic.groupby(['BusinessDate','PLU'])['StockedOut'].sum()).sort_values(by=['BusinessDate'],ascending=True).reset_index()

plt.figure(figsize=(15,6))
sns.set_style('whitegrid')
sns.boxplot(x='BusinessDate',y="StockedOut",data=df_temp)

df_store_18 = df_dynamic[df_dynamic['StoreID']==18]
df_store_18.head()

#Top 25% best seller products
# Most of the best selling items are Bake Case
df_temp_18 = pd.DataFrame(df_store_18.groupby(['PLU'])[['ReceivedQuantity','SoldQuantity','EndQuantity']].sum()).reset_index().sort_values('SoldQuantity',ascending=False)
# df_temp_18['Wastage'] = df_temp_18['ReceivedQuantity'] - (df_temp_18['SoldQuantity'] + df_temp_18['EndQuantity']) 
df_temp = df_temp_18.iloc[0:int(df_temp_18.shape[0]*0.25)]
df_temp = df_temp.merge(df_static,on=['PLU'],how='left')
df_temp['Sales in $'] = df_temp['SoldQuantity'].apply(lambda x:x*3)
df_temp[['PLU','Description','SoldQuantity','Sales in $','CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc']]

plt.figure(figsize=(12,6))
sns.set_color_codes("pastel")
sns.barplot(x="Sales in $", y="Description", data=df_temp,
            label="Total", color="b")
plt.ylabel("")

#Bottom 25% seller products
#Most of the least items are Cold Case
df_temp_18 = pd.DataFrame(df_store_18.groupby(['PLU'])[['ReceivedQuantity','SoldQuantity','EndQuantity']].sum()).reset_index().sort_values('SoldQuantity',ascending=False)
df_temp_18['Wastage'] = df_temp_18['ReceivedQuantity'] - (df_temp_18['SoldQuantity'] + df_temp_18['EndQuantity']) 
df_temp = df_temp_18.iloc[-int(df_temp_18.shape[0]*0.25):].sort_values('SoldQuantity',ascending=True)
df_temp = df_temp.merge(df_static,on=['PLU'],how='left')
df_temp['Sales in $'] = df_temp['SoldQuantity'].apply(lambda x:x*3)
df_temp[['PLU','Description','SoldQuantity','Sales in $','CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc']]

plt.figure(figsize=(12,6))
sns.set_color_codes("pastel")
sns.barplot(x="Sales in $", y="Description", data=df_temp,
            label="Total", color="b")
plt.ylabel("")

df_store_117 = df_dynamic[df_dynamic['StoreID']==117]
df_store_117.head()

#Top 25% best seller products
# Most of the best selling items are Bake Case
df_temp_117 = pd.DataFrame(df_store_117.groupby(['PLU'])[['ReceivedQuantity','SoldQuantity','EndQuantity']].sum()).reset_index().sort_values('SoldQuantity',ascending=False)
# df_temp_117['Wastage'] = df_temp_117['ReceivedQuantity'] - (df_temp_117['SoldQuantity'] + df_temp_117['EndQuantity']) 
df_temp = df_temp_117.iloc[0:int(df_temp_117.shape[0]*0.25)]
df_temp = df_temp.merge(df_static,on=['PLU'],how='left')
df_temp['Sales in $'] = df_temp['SoldQuantity'].apply(lambda x:x*3)
df_temp[['PLU','Description','SoldQuantity','Sales in $','CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc']]


plt.figure(figsize=(12,6))
sns.set_color_codes("pastel")
sns.barplot(x="Sales in $", y="Description", data=df_temp,
            label="Total", color="b")
plt.ylabel("")

#Bottom 25% seller products
#Most of the least selling items are Cold Case
df_temp_117 = pd.DataFrame(df_store_117.groupby(['PLU'])[['ReceivedQuantity','SoldQuantity','EndQuantity']].sum()).reset_index().sort_values('SoldQuantity',ascending=False)
# df_temp_117['Wastage'] = df_temp_117['ReceivedQuantity'] - (df_temp_117['SoldQuantity'] + df_temp_117['EndQuantity']) 
df_temp = df_temp_117.iloc[-int(df_temp_117.shape[0]*0.25):].sort_values('SoldQuantity',ascending=True)
df_temp = df_temp.merge(df_static,on=['PLU'],how='left')
df_temp['Sales in $'] = df_temp['SoldQuantity'].apply(lambda x:x*3)
df_temp[['PLU','Description','SoldQuantity','Sales in $','CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc']]

plt.figure(figsize=(12,6))
sns.set_color_codes("pastel")
sns.barplot(x="Sales in $", y="Description", data=df_temp,
            label="Total", color="b")
plt.ylabel("")

df_store_332 = df_dynamic[df_dynamic['StoreID']==332]
df_store_332.head()

#Top 25% best seller products
# Most of the best selling items are Bake Case
df_temp_332 = pd.DataFrame(df_store_332.groupby(['PLU'])[['ReceivedQuantity','SoldQuantity','EndQuantity']].sum()).reset_index().sort_values('SoldQuantity',ascending=False)
# df_temp_332['Wastage'] = df_temp_332['ReceivedQuantity'] - (df_temp_332['SoldQuantity'] + df_temp_332['EndQuantity']) 
df_temp = df_temp_332.iloc[0:int(df_temp_332.shape[0]*0.25)]
df_temp = df_temp.merge(df_static,on=['PLU'],how='left')
df_temp['Sales in $'] = df_temp['SoldQuantity'].apply(lambda x:x*3)
df_temp[['PLU','Description','SoldQuantity','Sales in $','CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc']]

plt.figure(figsize=(12,6))
sns.set_color_codes("pastel")
sns.barplot(x="Sales in $", y="Description", data=df_temp,
            label="Total", color="b")
plt.ylabel("")

#Bottom 25% seller products
#Most of the least selling items are Cold Case
df_temp_332 = pd.DataFrame(df_store_332.groupby(['PLU'])[['ReceivedQuantity','SoldQuantity','EndQuantity']].sum()).reset_index().sort_values('SoldQuantity',ascending=False)
# df_temp_332['Wastage'] = df_temp_332['ReceivedQuantity'] - (df_temp_332['SoldQuantity'] + df_temp_332['EndQuantity']) 
df_temp = df_temp_332.iloc[-int(df_temp_332.shape[0]*0.25):].sort_values('SoldQuantity',ascending=True)
df_temp = df_temp.merge(df_static,on=['PLU'],how='left')
df_temp['Sales in $'] = df_temp['SoldQuantity'].apply(lambda x:x*3)
df_temp[['PLU','Description','SoldQuantity','Sales in $','CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc']]

plt.figure(figsize=(12,6))
sns.set_color_codes("pastel")
sns.barplot(x="Sales in $", y="Description", data=df_temp,
            label="Total", color="b")
plt.ylabel("")

df_temp = df[(df['StoreID']==18) | (df['StoreID']==117) | (df['StoreID']==332)][['StoreID','BusinessDate','PLU','ReceivedQuantity','SoldQuantity','EndQuantity','StockedOut']].sort_values(['PLU','BusinessDate'])
df_temp

# df_temp['BusinessDate'].apply(lambda x:weekDay(int(x.split("-")[0]),int(x.split("-")[1]),int(x.split("-")[2])))
df_temp['DayOfMonth'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day)
df_temp['DayOfWeek'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day_of_week)
df_temp['Month'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).month)
df_temp['Year'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).year)

plt.figure(figsize=(15,6))
plt.ylabel('ReceivedQuantity')
sns.histplot(df_temp[df_temp['ReceivedQuantity']>0][['StoreID','ReceivedQuantity','DayOfMonth']].sort_values('DayOfMonth'),x='DayOfMonth',hue='StoreID', multiple="dodge")

plt.figure(figsize=(15,6))
plt.ylabel('SoldQuantity')
sns.histplot(df_temp[df_temp['SoldQuantity']>0][['StoreID','SoldQuantity','DayOfMonth']].sort_values('DayOfMonth'),x='DayOfMonth',hue='StoreID', multiple="dodge")

plt.figure(figsize=(15,6))
plt.ylabel('EndQuantity')
sns.histplot(df_temp[df_temp['EndQuantity']>0][['StoreID','EndQuantity','DayOfMonth']].sort_values('DayOfMonth'),x='DayOfMonth',hue='StoreID', multiple="dodge")

plt.figure(figsize=(15,6))
plt.ylabel('StockedOut')
sns.histplot(df_temp[df_temp['StockedOut']==1][['StoreID','StockedOut','DayOfMonth']].sort_values('DayOfMonth'),x='DayOfMonth',hue='StoreID', multiple="dodge")

df_temp = df[(df['StoreID']==18) | (df['StoreID']==117) | (df['StoreID']==332) ][['StoreID','BusinessDate','PLU','ReceivedQuantity','SoldQuantity','EndQuantity','StockedOut']].sort_values(['PLU','BusinessDate'])

# df_temp['BusinessDate'].apply(lambda x:weekDay(int(x.split("-")[0]),int(x.split("-")[1]),int(x.split("-")[2])))
df_temp['DayOfMonth'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day)
df_temp['DayOfWeek'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day_of_week)
df_temp['Month'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).month)
df_temp['Year'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).year)
#adding holiday
from pandas.tseries.holiday import USFederalHolidayCalendar as calendar
cal = calendar()
holidays = cal.holidays(start = df['BusinessDate'].min(),end=df['BusinessDate'].max())
df_temp['Holiday'] = (df_temp['BusinessDate'].isin(holidays)).astype(int)
# Stockout trend at Day of Week level
plt.figure(figsize=(15,5))
sns.histplot(df_temp[df_temp['StockedOut']==1][['StoreID','StockedOut','DayOfWeek']].sort_values('DayOfWeek'),x='DayOfWeek',hue='StoreID', multiple="dodge")

# Stockout trend at Month Level
plt.figure(figsize=(15,5))
sns.histplot(df_temp[df_temp['StockedOut']==1][['StoreID','StockedOut','Month']].sort_values('Month'),x='Month',hue='StoreID',multiple="dodge")

# Stockout trend at Day of Month level
plt.figure(figsize=(15,5))
sns.histplot(df_temp[df_temp['StockedOut']==1][['StoreID','StockedOut','DayOfMonth']].sort_values('DayOfMonth'),x='DayOfMonth',hue='StoreID', multiple="dodge")

df_temp = df[df['StoreID']==117][['BusinessDate','PLU','ReceivedQuantity','SoldQuantity','EndQuantity','StockedOut']].sort_values(['PLU','BusinessDate'])

# df_temp['BusinessDate'].apply(lambda x:weekDay(int(x.split("-")[0]),int(x.split("-")[1]),int(x.split("-")[2])))
df_temp['DayOfMonth'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day)
df_temp['DayOfWeek'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day_of_week)
df_temp['Month'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).month)
df_temp['Year'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).year)

# Stockout trend at Day of Week level for StoreID 117
plt.figure(figsize=(12,6))
sns.histplot(df_temp[df_temp['StockedOut']==1][['StockedOut','DayOfWeek']].sort_values('DayOfWeek'),x='DayOfWeek',)

# Stockout trend at Month Level StoreID 117
plt.figure(figsize=(12,6))
sns.histplot(df_temp[df_temp['StockedOut']==1][['StockedOut','Month']].sort_values('Month'),x='Month',)

# Stockout trend at Day of Month level StoreID 117
plt.figure(figsize=(12,6))
sns.histplot(df_temp[df_temp['StockedOut']==1][['StockedOut','DayOfMonth']].sort_values('DayOfMonth'),x='DayOfMonth',)

df_temp = df[df['StoreID']==332][['BusinessDate','PLU','ReceivedQuantity','SoldQuantity','EndQuantity','StockedOut']].sort_values(['PLU','BusinessDate'])

# df_temp['BusinessDate'].apply(lambda x:weekDay(int(x.split("-")[0]),int(x.split("-")[1]),int(x.split("-")[2])))
df_temp['DayOfMonth'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day)
df_temp['DayOfWeek'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day_of_week)
df_temp['Month'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).month)
df_temp['Year'] = df_temp['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).year)

# Stockout trend at Day of Week level for StoreID 332
plt.figure(figsize=(12,6))
sns.histplot(df_temp[df_temp['StockedOut']==1][['StockedOut','DayOfWeek']].sort_values('DayOfWeek'),x='DayOfWeek',)

# Stockout trend at Month Level StoreID 332
plt.figure(figsize=(12,6))
sns.histplot(df_temp[df_temp['StockedOut']==1][['StockedOut','Month']].sort_values('Month'),x='Month',)

# Stockout trend at Day of Month level StoreID 332
plt.figure(figsize=(12,6))
sns.histplot(df_temp[df_temp['StockedOut']==1][['StockedOut','DayOfMonth']].sort_values('DayOfMonth'),x='DayOfMonth',)

df_1 = df.drop(['CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc','ItemType'],axis=1)

df_1['Months'] = pd.DatetimeIndex(df_1['BusinessDate']).month
df_1['Year'] = pd.DatetimeIndex(df_1['BusinessDate']).year
df_1

fh_18 = df_1.loc[(df['StoreID'] ==18)]
fh_117 = df_1.loc[(df['StoreID'] ==117)]
fh_332 = df_1.loc[(df['StoreID'] ==332)]
# 3 Store data  :
df_a=pd.concat([fh_18,fh_117,fh_332])
df_a = df_a.sort_values(by=['Description'])

df_a = df_a[df_a['Description']>="Everything Bagel"]

g = sns.FacetGrid(df_a, col="Months", sharex=False)
g.map(sns.boxplot, 'ReceivedQuantity', 'Description')

df_a["BusinessDate"] = pd.to_datetime(df_a["BusinessDate"])
df_a["DayOfWeek"] = df_a["BusinessDate"].dt.weekday
df_a["IsWeekend"] = df_a["BusinessDate"].dt.weekday >= 5

df_b = df_a.pivot_table(index='Description', columns='IsWeekend', values='SoldQuantity',aggfunc='sum')
df_b.plot(kind='bar', figsize=(15, 8), color=['red', 'Green', '#e37827', '#275444'])

df_b = df_a.pivot_table(index='StoreID', columns='Description', values='ReceivedQuantity',aggfunc='sum')
df_b.plot(kind='bar', figsize=(15, 8), color=['red', 'Green', '#e37827', '#275444'])

df_a_groupby=df_a.groupby(['Description','StoreID'])['SoldQuantity'].sum().reset_index(name='Total_Sales')
df_H = df_a_groupby.pivot(index='Description', columns='StoreID', values='Total_Sales')
df_H.plot(kind='bar', figsize=(15, 5), color=['blue', 'red', 'green'])

df_G = df_a.groupby(['Description','DayOfWeek'])['SoldQuantity'].sum().reset_index(name='Total_SQ')
df_H = df_G.pivot(index='Description', columns='DayOfWeek', values='Total_SQ')
df_H.plot(kind='bar', figsize=(15, 5), color=['blue', 'red', 'green','orange','purple','black','cyan'])

df_G = df_a.groupby(['Description','DayOfWeek'])['EndQuantity'].sum().reset_index(name='Total_SQ')
df_H = df_G.pivot(index='Description', columns='DayOfWeek', values='Total_SQ')
df_H.plot(kind='bar', figsize=(15, 5), color=['blue', 'red', 'green','orange','purple','black','cyan'])

df_G = df_a.groupby(['Description','Months'])['ReceivedQuantity'].sum().reset_index(name='Total_SQ')
df_H = df_G.pivot(index='Description', columns='Months', values='Total_SQ')
df_H.plot(kind='bar', figsize=(15, 5), color=['blue', 'red', 'green','orange','purple','black','cyan'])

"""# Section 2:  Prediction"""

!pip install pyjanitor

import janitor

df = pd.read_csv("/content/drive/MyDrive/Coffee_Stores_Data.csv")

df=df.case_when(
df.BusinessDate.eq('2019-06-29') ,  66.65 ,
df.BusinessDate.eq('2019-06-30') ,	69.06 ,
df.BusinessDate.eq('2019-07-01') ,	68.60 ,
df.BusinessDate.eq('2019-07-02') ,	68.06 ,
df.BusinessDate.eq('2019-07-03') ,	67.19 ,
df.BusinessDate.eq('2019-07-04') ,	67.82 ,
df.BusinessDate.eq('2019-07-05') ,	69.84 ,
df.BusinessDate.eq('2019-07-06') ,	70.64 ,
df.BusinessDate.eq('2019-07-07') ,	68.66 ,
df.BusinessDate.eq('2019-07-08') ,	66.56 ,
df.BusinessDate.eq('2019-07-09') ,	67.64 ,
df.BusinessDate.eq('2019-07-10') ,	70.49 ,
df.BusinessDate.eq('2019-07-11') ,	72.96 ,
df.BusinessDate.eq('2019-07-12') ,	74.65 ,
df.BusinessDate.eq('2019-07-13') ,  75.14 ,
df.BusinessDate.eq('2019-07-14') ,	74.47 ,
df.BusinessDate.eq('2019-07-15') ,	74.32 ,
df.BusinessDate.eq('2019-07-16') ,	72.79 ,
df.BusinessDate.eq('2019-07-17') ,	72.08 ,
df.BusinessDate.eq('2019-07-18') ,	72.24 ,
df.BusinessDate.eq('2019-07-19') ,	71.11 ,
df.BusinessDate.eq('2019-07-20') ,	71.23 ,
df.BusinessDate.eq('2019-07-21') ,	73.37 ,
df.BusinessDate.eq('2019-07-22') ,	75.68 ,
df.BusinessDate.eq('2019-07-23') ,	76.65 ,
df.BusinessDate.eq('2019-07-24') ,	77.29 ,
df.BusinessDate.eq('2019-07-25') ,	77.62 ,
df.BusinessDate.eq('2019-07-26') ,	76.86 ,
df.BusinessDate.eq('2019-07-27') ,	77.58 ,
df.BusinessDate.eq('2019-07-28') ,	78.16 ,
df.BusinessDate.eq('2019-07-29') ,	75.19 ,
df.BusinessDate.eq('2019-07-30') ,	73.04 ,
df.BusinessDate.eq('2019-07-31') ,	72.67 ,
df.BusinessDate.eq('2019-08-01') ,	72.94 ,
df.BusinessDate.eq('2019-08-02') ,	73.65 ,
df.BusinessDate.eq('2019-08-03') ,	75.39 ,
df.BusinessDate.eq('2019-08-04') ,	75.86 ,
df.BusinessDate.eq('2019-08-05') ,	75.78 ,
df.BusinessDate.eq('2019-08-06') ,	76.50 ,
df.BusinessDate.eq('2019-08-07') ,	74.43 ,
df.BusinessDate.eq('2019-08-08') ,	71.63 ,
df.BusinessDate.eq('2019-08-09') ,	69.56 ,
df.BusinessDate.eq('2019-08-10') ,	67.28 ,
df.BusinessDate.eq('2019-08-11') ,	68.11 ,
df.BusinessDate.eq('2019-08-12') ,	71.19 ,
df.BusinessDate.eq('2019-08-13') ,	74.06 ,
df.BusinessDate.eq('2019-08-14') ,	77.21 ,
df.BusinessDate.eq('2019-08-15') ,	78.54 ,
df.BusinessDate.eq('2019-08-16') ,	76.68 ,
df.BusinessDate.eq('2019-08-17') ,	73.37 ,
df.BusinessDate.eq('2019-08-18') ,	71.34 ,
df.BusinessDate.eq('2019-08-19') ,	70.90 ,
df.BusinessDate.eq('2019-08-20') ,	71.78 ,
df.BusinessDate.eq('2019-08-21') ,	73.29 ,
df.BusinessDate.eq('2019-08-22') ,	73.98 ,
df.BusinessDate.eq('2019-08-23') ,	74.37 ,
df.BusinessDate.eq('2019-08-24') ,	75.46 ,
df.BusinessDate.eq('2019-08-25') ,	76.51 ,
df.BusinessDate.eq('2019-08-26') ,	77.68 ,
df.BusinessDate.eq('2019-08-27') ,	78.19 ,
df.BusinessDate.eq('2019-08-28') ,	76.47 ,
df.BusinessDate.eq('2019-08-29') ,	74.35 ,
df.BusinessDate.eq('2019-08-30') ,	73.93 ,
df.BusinessDate.eq('2019-08-31') ,	75.13 ,
df.BusinessDate.eq('2019-09-01') ,	75.81 ,
df.BusinessDate.eq('2019-09-02') ,	76.32 ,
df.BusinessDate.eq('2019-09-03') ,	75.87 ,
df.BusinessDate.eq('2019-09-04') ,	76.69 ,
df.BusinessDate.eq('2019-09-05') ,	75.76 ,
df.BusinessDate.eq('2019-09-06') ,	74.14 ,
df.BusinessDate.eq('2019-09-07') ,	70.42 ,
df.BusinessDate.eq('2019-09-08') ,	66.21 ,
df.BusinessDate.eq('2019-09-09') ,	65.09 ,
df.BusinessDate.eq('2019-09-10') ,	63.36 ,
df.BusinessDate.eq('2019-09-11') ,	64.75 ,
df.BusinessDate.eq('2019-09-12') ,	70.24 ,
df.BusinessDate.eq('2019-09-13') ,	74.02 ,
df.BusinessDate.eq('2019-09-14') ,	74.63 ,
df.BusinessDate.eq('2019-09-15') ,	71.09 ,
df.BusinessDate.eq('2019-09-16') ,	64.61 ,
df.BusinessDate.eq('2019-09-17') ,	62.18 ,
df.BusinessDate.eq('2019-09-18') ,	61.91 ,
df.BusinessDate.eq('2019-09-19') ,	60.83 ,
df.BusinessDate.eq('2019-09-20') ,	62.12 ,
df.BusinessDate.eq('2019-09-21') ,	65.57 ,
df.BusinessDate.eq('2019-09-22') ,	67.00 ,
df.BusinessDate.eq('2019-09-23') ,	66.65 ,
df.BusinessDate.eq('2019-09-24') ,	70.83 ,
df.BusinessDate.eq('2019-09-25') ,	73.32 ,
df.BusinessDate.eq('2019-09-26') ,	70.16 ,
df.BusinessDate.eq('2019-09-27') ,	65.04 ,
df.BusinessDate.eq('2019-09-28') ,	57.41 ,
df.BusinessDate.eq('2019-09-29') ,	52.56 ,
df.BusinessDate.eq('2019-09-30') ,	51.84 ,
df.BusinessDate.eq('2019-10-01') ,	52.78 ,
df.BusinessDate.eq('2019-10-02') ,	56.37 ,
df.BusinessDate.eq('2019-10-03') ,	58.12 ,
df.BusinessDate.eq('2019-10-04') ,	57.60 ,
df.BusinessDate.eq('2019-10-05') ,	60.94 ,
df.BusinessDate.eq('2019-10-06') ,	64.79 ,
df.BusinessDate.eq('2019-10-07') ,	66.78 ,
df.BusinessDate.eq('2019-10-08') ,	64.94 ,
df.BusinessDate.eq('2019-10-09') ,	59.40 ,
df.BusinessDate.eq('2019-10-10') ,	59.92 ,
df.BusinessDate.eq('2019-10-11') ,	58.57 ,
df.BusinessDate.eq('2019-10-12') ,	59.35 ,
df.BusinessDate.eq('2019-10-13') ,	58.65 ,
df.BusinessDate.eq('2019-10-14') ,	58.46 ,
df.BusinessDate.eq('2019-10-15') ,	60.75 ,
df.BusinessDate.eq('2019-10-16') ,	61.58 ,
df.BusinessDate.eq('2019-10-17') ,	57.50 ,
df.BusinessDate.eq('2019-10-18') ,	56.02 ,
df.BusinessDate.eq('2019-10-19') ,	56.04 ,
df.BusinessDate.eq('2019-10-20') ,	57.88 ,
df.BusinessDate.eq('2019-10-21') ,	62.62 ,
df.BusinessDate.eq('2019-10-22') ,	65.49 ,
df.BusinessDate.eq('2019-10-23') ,	65.19 ,
df.BusinessDate.eq('2019-10-24') ,	66.5 ,
df.BusinessDate.eq('2019-10-25') ,	66.72 ,
df.BusinessDate.eq('2019-10-26') ,	62.87 ,
df.BusinessDate.eq('2019-10-27') ,	53.70 ,
df.BusinessDate.eq('2019-10-28') ,	51.61 ,
df.BusinessDate.eq('2019-10-29') ,	49.77 ,
df.BusinessDate.eq('2019-10-30') ,	48.92 ,
df.BusinessDate.eq('2019-10-31') ,	51.29 ,
df.BusinessDate.eq('2019-11-01') ,	54.44 ,
df.BusinessDate.eq('2019-11-02') ,	57.60 ,
df.BusinessDate.eq('2019-11-03') ,	58.92 ,
df.BusinessDate.eq('2019-11-04') ,	59.42 ,
df.BusinessDate.eq('2019-11-05') ,	59.86 ,
df.BusinessDate.eq('2019-11-06') ,	59.53 ,
df.BusinessDate.eq('2019-11-07') ,	59.24 ,
df.BusinessDate.eq('2019-11-08') ,	60.39 ,
df.BusinessDate.eq('2019-11-09') ,	60.58 ,
df.BusinessDate.eq('2019-11-10') ,	59.35 ,
df.BusinessDate.eq('2019-11-11') ,	59.79 ,
df.BusinessDate.eq('2019-11-12') ,	60.33 ,
df.BusinessDate.eq('2019-11-13') ,	58.62 ,
df.BusinessDate.eq('2019-11-14') ,	57.37 ,
df.BusinessDate.eq('2019-11-15') ,	55.80 ,
df.BusinessDate.eq('2019-11-16') ,	57.22 ,
df.BusinessDate.eq('2019-11-17') ,	60.67 ,
df.BusinessDate.eq('2019-11-18') ,	61.37 ,
df.BusinessDate.eq('2019-11-19') ,	56.28 ,
df.BusinessDate.eq('2019-11-20') ,	49.55 ,
df.BusinessDate.eq('2019-11-21') ,	47.99 ,
df.BusinessDate.eq('2019-11-22') ,	48.74 ,
df.BusinessDate.eq('2019-11-23') ,	50.69 ,
df.BusinessDate.eq('2019-11-24') ,	51.60 ,
df.BusinessDate.eq('2019-11-25') ,	47.03 ,
df.BusinessDate.eq('2019-11-26') ,	41.49 ,
df.BusinessDate.eq('2019-11-27') ,	39.37 ,
df.BusinessDate.eq('2019-11-28') ,	37.31 ,
df.BusinessDate.eq('2019-11-29') ,	36.61 ,
df.BusinessDate.eq('2019-11-30') ,	37.22 ,
df.BusinessDate.eq('2019-12-01') ,	43.85 ,
df.BusinessDate.eq('2019-12-02') ,	48.05 ,
df.BusinessDate.eq('2019-12-03') ,	49.29 ,
df.BusinessDate.eq('2019-12-04') ,	47.74 ,
df.BusinessDate.eq('2019-12-05') ,	48.27 ,
df.BusinessDate.eq('2019-12-06') ,	50.05 ,
df.BusinessDate.eq('2019-12-07') ,	50.94 ,
df.BusinessDate.eq('2019-12-08') ,	48.96 ,
df.BusinessDate.eq('2019-12-09') ,	47.08 ,
df.BusinessDate.eq('2019-12-10') ,	46.56 ,
df.BusinessDate.eq('2019-12-11') ,	47.77 ,
df.BusinessDate.eq('2019-12-12') ,	52.00 ,
df.BusinessDate.eq('2019-12-13') ,	51.16 ,
df.BusinessDate.eq('2019-12-14') ,	46.75 ,
df.BusinessDate.eq('2019-12-15') ,	42.38 ,
df.BusinessDate.eq('2019-12-16') ,	41.44 ,
df.BusinessDate.eq('2019-12-17') ,	41.97 ,
df.BusinessDate.eq('2019-12-18') ,	42.15 ,
df.BusinessDate.eq('2019-12-19') ,	44.73 ,
df.BusinessDate.eq('2019-12-20') ,	48.67 ,
df.BusinessDate.eq('2019-12-21') ,	49.69 ,
df.BusinessDate.eq('2019-12-22') ,	46.81 ,
df.BusinessDate.eq('2019-12-23') ,	42.87 ,
df.BusinessDate.eq('2019-12-24') ,	40.61 ,
df.BusinessDate.eq('2019-12-25') ,	40.35 ,
df.BusinessDate.eq('2019-12-26') ,	39.71 ,
df.BusinessDate.eq('2019-12-27') ,	39.80 ,
df.BusinessDate.eq('2019-12-28') ,	41.99 ,
df.BusinessDate.eq('2019-12-29') ,  42.47 ,
df.BusinessDate.eq('2019-12-30') ,	43.71 ,
df.BusinessDate.eq('2019-12-31') ,	46.64 ,
df.BusinessDate.eq('2020-01-01') ,	47.34 ,
df.BusinessDate.eq('2020-01-02') ,	47.66 ,
df.BusinessDate.eq('2020-01-03') ,	49.66 ,
df.BusinessDate.eq('2020-01-04') ,	48.19 ,
df.BusinessDate.eq('2020-01-05') ,	45.71 ,
df.BusinessDate.eq('2020-01-06') ,	47.10 ,
df.BusinessDate.eq('2020-01-07') ,	46.46 ,
df.BusinessDate.eq('2020-01-08') ,	43.50 ,
df.BusinessDate.eq('2020-01-09') ,	41.73 ,
df.BusinessDate.eq('2020-01-10') ,	42.59 ,
df.BusinessDate.eq('2020-01-11') ,	42.63 ,
df.BusinessDate.eq('2020-01-12') ,	41.78 ,
df.BusinessDate.eq('2020-01-13') ,	42.52 ,
df.BusinessDate.eq('2020-01-14') ,	41.94 ,
df.BusinessDate.eq('2020-01-15') ,	41.16 ,
df.BusinessDate.eq('2020-01-16') ,	41.19 ,
df.BusinessDate.eq('2020-01-17') ,	40.37 ,
df.BusinessDate.eq('2020-01-18') ,	43.45 ,
df.BusinessDate.eq('2020-01-19') ,	46.44 ,
df.BusinessDate.eq('2020-01-20') ,	47.21 ,
df.BusinessDate.eq('2020-01-21') ,	45.98 ,
df.BusinessDate.eq('2020-01-22') ,	48.02 ,
df.BusinessDate.eq('2020-01-23') ,	49.43 ,
df.BusinessDate.eq('2020-01-24') ,	50.38 ,
df.BusinessDate.eq('2020-01-25') ,	51.16 ,
df.BusinessDate.eq('2020-01-26') ,	49.69 ,
df.BusinessDate.eq('2020-01-27') ,	49.17 ,
df.BusinessDate.eq('2020-01-28') ,	48.66 ,
df.BusinessDate.eq('2020-01-29') ,	47.62 ,
df.BusinessDate.eq('2020-01-30') ,	50.38 ,
df.BusinessDate.eq('2020-01-31') ,	53.53 ,
df.BusinessDate.eq('2020-02-01') ,	55.03 ,
df.BusinessDate.eq('2020-02-02') ,	47.51 ,
df.BusinessDate.eq('2020-02-03') ,	39.17 ,
df.BusinessDate.eq('2020-02-04') ,	37.57 ,
df.BusinessDate.eq('2020-02-05') ,	42.64 ,
df.BusinessDate.eq('2020-02-06') ,	48.34 ,
df.BusinessDate.eq('2020-02-07') ,	50.59 ,
df.BusinessDate.eq('2020-02-08') ,	48.55 ,
df.BusinessDate.eq('2020-02-09') ,	45.18 ,
df.BusinessDate.eq('2020-02-10') ,	48.35 ,
df.BusinessDate.eq('2020-02-11') ,	50.10 ,
df.BusinessDate.eq('2020-02-12') ,	49.15 ,
df.BusinessDate.eq('2020-02-13') ,	48.45 ,
df.BusinessDate.eq('2020-02-14') ,	48.75 ,
df.BusinessDate.eq('2020-02-15') ,	49.44 ,
df.BusinessDate.eq('2020-02-16') ,	51.21 ,
df.BusinessDate.eq('2020-02-17') ,	50.67 ,
df.BusinessDate.eq('2020-02-18') ,	49.89 ,
df.BusinessDate.eq('2020-02-19') ,	49.12 ,
df.BusinessDate.eq('2020-02-20') ,	51.04 ,
df.BusinessDate.eq('2020-02-21') ,	53.27 ,
df.BusinessDate.eq('2020-02-22') ,	51.06 ,
df.BusinessDate.eq('2020-02-23') ,	49.71 ,
df.BusinessDate.eq('2020-02-24') ,	51.19 ,
df.BusinessDate.eq('2020-02-25') ,	54.39 ,
df.BusinessDate.eq('2020-02-26') ,	55.22 ,
df.BusinessDate.eq('2020-02-27') ,	56.65 ,
df.BusinessDate.eq('2020-02-28') ,	57.58 ,
df.BusinessDate.eq('2020-02-29') ,	51.64 ,
df.BusinessDate.eq('2020-03-01') ,	45.29 ,
df.BusinessDate.eq('2020-03-02') ,	47.37 ,
df.BusinessDate.eq('2020-03-03') ,	54.10 ,
df.BusinessDate.eq('2020-03-04') ,	55.83 ,
df.BusinessDate.eq('2020-03-05') ,	56.05 ,
df.BusinessDate.eq('2020-03-06') ,	54.41 ,
df.BusinessDate.eq('2020-03-07') ,	48.57 ,
df.BusinessDate.eq('2020-03-08') ,	46.16 ,
df.BusinessDate.eq('2020-03-09') ,	49.34 ,
df.BusinessDate.eq('2020-03-10') ,	52.13 ,
df.BusinessDate.eq('2020-03-11') ,	53.91 ,
df.BusinessDate.eq('2020-03-12') ,	53.48 ,
df.BusinessDate.eq('2020-03-13') ,	50.67 ,
df.BusinessDate.eq('2020-03-14') ,	47.09 ,
df.BusinessDate.eq('2020-03-15') ,	44.76 ,
df.BusinessDate.eq('2020-03-16') ,	43.12 ,
df.BusinessDate.eq('2020-03-17') ,	41.23 ,
df.BusinessDate.eq('2020-03-18') ,	41.75 ,
df.BusinessDate.eq('2020-03-19') ,	43.50 ,
df.BusinessDate.eq('2020-03-20') ,	45.50 ,
df.BusinessDate.eq('2020-03-21') ,	48.52 ,
df.BusinessDate.eq('2020-03-22') ,	50.13 ,
df.BusinessDate.eq('2020-03-23') ,	48.58 ,
df.BusinessDate.eq('2020-03-24') ,	46.32 ,
df.BusinessDate.eq('2020-03-25') ,	43.76 ,
df.BusinessDate.eq('2020-03-26') ,	41.93 ,
df.BusinessDate.eq('2020-03-27') ,	42.98 ,
df.BusinessDate.eq('2020-03-28') ,	45.22 ,
df.BusinessDate.eq('2020-03-29') ,	47.66 ,
df.BusinessDate.eq('2020-03-30') ,	49.94 ,
df.BusinessDate.eq('2020-03-31') ,	53.23 ,
df.BusinessDate.eq('2020-04-01') ,	51.09 ,
df.BusinessDate.eq('2020-04-02') ,	49.17 ,
df.BusinessDate.eq('2020-04-03') ,	49.02 ,
df.BusinessDate.eq('2020-04-04') ,	48.22 ,
df.BusinessDate.eq('2020-04-05') ,	47.14 ,
df.BusinessDate.eq('2020-04-06') ,	45.88 ,
df.BusinessDate.eq('2020-04-07') ,	48.27 ,
df.BusinessDate.eq('2020-04-08') ,	50.26 ,
df.BusinessDate.eq('2020-04-09') ,	49.39 ,
df.BusinessDate.eq('2020-04-10') ,	50.81 ,
df.BusinessDate.eq('2020-04-11') ,	53.67 ,
df.BusinessDate.eq('2020-04-12') ,	53.99 ,
df.BusinessDate.eq('2020-04-13') ,	53.29 ,
df.BusinessDate.eq('2020-04-14') ,	55.29 ,
df.BusinessDate.eq('2020-04-15') ,	58.09 ,
df.BusinessDate.eq('2020-04-16') ,	57.50 ,
df.BusinessDate.eq('2020-04-17') ,	55.01 ,
df.BusinessDate.eq('2020-04-18') ,	53.37 ,
df.BusinessDate.eq('2020-04-19') ,	53.65 ,
df.BusinessDate.eq('2020-04-20') ,	53.58 ,
df.BusinessDate.eq('2020-04-21') ,	55.77 ,
df.BusinessDate.eq('2020-04-22') ,	59.69 ,
df.BusinessDate.eq('2020-04-23') ,	63.45 ,
df.BusinessDate.eq('2020-04-24') ,	65.84 ,
df.BusinessDate.eq('2020-04-25') ,	66.84 ,
df.BusinessDate.eq('2020-04-26') ,	65.25 ,
df.BusinessDate.eq('2020-04-27') ,	64.08 ,
df.BusinessDate.eq('2020-04-28') ,	67.28 ,
df.BusinessDate.eq('2020-04-29') ,	66.79 ,
df.BusinessDate.eq('2020-04-30') ,	64.20 ,
df.BusinessDate.eq('2020-05-01') ,	61.52 ,
df.BusinessDate.eq('2020-05-02') ,	59.75 ,
df.BusinessDate.eq('2020-05-03') ,	58.42 ,
df.BusinessDate.eq('2020-05-04') ,	60.33 ,
df.BusinessDate.eq('2020-05-05') ,	63.34 ,
df.BusinessDate.eq('2020-05-06') ,	65.00 ,
df.BusinessDate.eq('2020-05-07') ,	67.31 ,
df.BusinessDate.eq('2020-05-08') ,	69.16 ,
df.BusinessDate.eq('2020-05-09') ,	68.82 ,
df.BusinessDate.eq('2020-05-10') ,	65.89 ,
df.BusinessDate.eq('2020-05-11') ,	60.83 ,
df.BusinessDate.eq('2020-05-12') ,	56.39 ,
df.BusinessDate.eq('2020-05-13') ,	54.92 ,
df.BusinessDate.eq('2020-05-14') ,	56.72 ,
df.BusinessDate.eq('2020-05-15') ,	59.82 ,
df.BusinessDate.eq('2020-05-16') ,	62.69 ,
df.BusinessDate.eq('2020-05-17') ,	61.00 ,
df.BusinessDate.eq('2020-05-18') ,	55.74 ,
df.BusinessDate.eq('2020-05-19') ,	53.40 ,
df.BusinessDate.eq('2020-05-20') ,	55.89 ,
df.BusinessDate.eq('2020-05-21') ,	60.12 ,
df.BusinessDate.eq('2020-05-22') ,	59.45 ,
df.BusinessDate.eq('2020-05-23') ,	60.24 ,
df.BusinessDate.eq('2020-05-24') ,	64.69 ,
df.BusinessDate.eq('2020-05-25') ,	70.01 ,
df.BusinessDate.eq('2020-05-26') ,	73.52 ,
df.BusinessDate.eq('2020-05-27') ,	74.72 ,
df.BusinessDate.eq('2020-05-28') ,	74.46 ,
df.BusinessDate.eq('2020-05-29') ,	70.89 ,
df.BusinessDate.eq('2020-05-30') ,	63.57 ,
df.BusinessDate.eq('2020-05-31') ,	62.46 ,
df.BusinessDate.eq('2020-06-01') ,	65.99 ,
df.BusinessDate.eq('2020-06-02') ,	70.34 ,
df.BusinessDate.eq('2020-06-03') ,	74.90 ,
df.BusinessDate.eq('2020-06-04') ,	73.52 ,
df.BusinessDate.eq('2020-06-05') ,	66.33 ,
df.BusinessDate.eq('2020-06-06') ,	59.46 ,
df.BusinessDate.eq('2020-06-07') ,	55.80 ,
df.BusinessDate.eq('2020-06-08') ,	59.13 ,
df.BusinessDate.eq('2020-06-09') ,  65.93 ,
df.BusinessDate.eq('2020-06-10') ,	70.65 ,
df.BusinessDate.eq('2020-06-11') ,	71.27 ,
df.BusinessDate.eq('2020-06-12') ,	64.66 ,
df.BusinessDate.eq('2020-06-13') ,	60.12 ,
df.BusinessDate.eq('2020-06-14') ,	62.91 ,
df.BusinessDate.eq('2020-06-15') ,	64.94 ,
df.BusinessDate.eq('2020-06-16') ,	63.06 ,
df.BusinessDate.eq('2020-06-17') ,	64.16 ,
df.BusinessDate.eq('2020-06-18') ,	67.84 ,
df.BusinessDate.eq('2020-06-19') ,	70.20 ,
df.BusinessDate.eq('2020-06-20') ,	71.16 ,
df.BusinessDate.eq('2020-06-21') ,	71.87 ,
df.BusinessDate.eq('2020-06-22') ,	74.26 ,
df.BusinessDate.eq('2020-06-23') ,	75.77 ,
df.BusinessDate.eq('2020-06-24') ,	75.20 ,
df.BusinessDate.eq('2020-06-25') ,	74.67 ,
df.BusinessDate.eq('2020-06-26') ,	75.76 ,
df.BusinessDate.eq('2020-06-27') ,	74.33 ,
df.BusinessDate.eq('2020-06-28') ,	66.33 ,
df.BusinessDate.eq('2020-06-29') ,	63.99 ,
75,
column_name='Temperature')

df.head()

df_18 = df[(((df['StoreID']==18) | (df['StoreID']==117) | (df['StoreID']==332)))].copy()
# df_18['DayOfMonth'] = df_18['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day).astype(str)
df_18['DayOfWeek'] = df_18['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day_of_week).astype(str)
df_18['Month'] = df_18['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).month).astype(str)
df_18['Year'] = df_18['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).year).astype(str)
df_18['StoreID'] = df_18['StoreID'].astype(str)
df_18['PLU'] = df_18['PLU'].astype(str)

date_time = pd.to_datetime(df_18['BusinessDate'], format='%Y.%m.%d')
train_dates = pd.to_datetime(df_18['BusinessDate'])

df_18.drop(['BusinessDate','Description','CategoryLvl1Desc','ItemType', 'GroupID'],axis=1,inplace=True)

df_18.columns

date_time

df_18 = pd.get_dummies(df_18,['StoreID', 'PLU', 'CategoryLvl2Desc', 'CategoryLvl3Desc','DayOfWeek', 'Month', 'Year'])

df_18 = df_18.astype(float)

# plot_cols = ['ReceivedQuantity', 'SoldQuantity', 'EndQuantity']
# plot_features = df_18[plot_cols]
# plot_features.index = date_time
# _ = plot_features.plot(subplots=True)

# plot_features = df[plot_cols][:1000]
# plot_features.index = date_time[:1000]
# _ = plot_features.plot(subplots=True)

df_18.loc[df_18['MissedSales'].isna(),'MissedSales']=0.0

fig, ax = plt.subplots(figsize=(16,6))

df_temp = df_18[['ReceivedQuantity', 'MissedSales',
       'CategoryLvl3Desc_Bagels', 'CategoryLvl3Desc_Cakes & Breads',
       'CategoryLvl3Desc_Cookies & Bars', 'CategoryLvl3Desc_Muffins & Scones',
       'CategoryLvl3Desc_Pastries', 'CategoryLvl3Desc_Ready to Drink','Temperature']]
corrMatrix = df_temp.corr()
sns.heatmap(corrMatrix, annot=True)
plt.show()

df_18 = df_18[['StoreID_117', 'StoreID_18', 'StoreID_332',
       'PLU_12216', 'PLU_3000024', 'PLU_3000096', 'PLU_3000159', 'PLU_3000162',
       'PLU_3000166', 'PLU_3000181', 'PLU_3000207', 'PLU_3000209',
       'PLU_3000210', 'PLU_3000211', 'PLU_3000212', 'PLU_3000214',
       'PLU_3000227', 'PLU_3000277', 'PLU_3000278', 'PLU_3000281',
       'PLU_3000293', 'PLU_3000296', 'PLU_3000297', 'PLU_6228', 'PLU_810407',
       'PLU_810703', 'PLU_820221', 'PLU_820224', 'PLU_820602', 'PLU_820801',
       'PLU_820902', 'PLU_830620', 'PLU_841402', 'PLU_851004','SoldQuantity','ReceivedQuantity','MissedSales',
       'CategoryLvl3Desc_Bagels', 'CategoryLvl3Desc_Cakes & Breads',
       'CategoryLvl3Desc_Cookies & Bars', 'CategoryLvl3Desc_Muffins & Scones',
       'CategoryLvl3Desc_Pastries', 'CategoryLvl3Desc_Ready to Drink',
       'DayOfWeek_0', 'DayOfWeek_1',
       'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5',
       'DayOfWeek_6', 'Month_1', 'Month_10', 'Month_11', 'Month_12', 'Month_2',
       'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8',
       'Month_9', 'Year_2019', 'Year_2020','Temperature']]

# df_18_X = df_18.drop(['SoldQuantity'],axis=1)
# df_18_y = df_18['SoldQuantity']

from sklearn.preprocessing import StandardScaler,MinMaxScaler

scaler = StandardScaler()

# df_18_t = scaler.fit_transform(
#     df_18[['StoreID_117', 'StoreID_18', 'StoreID_332',
#        'PLU_12216', 'PLU_3000024', 'PLU_3000096', 'PLU_3000159', 'PLU_3000162',
#        'PLU_3000166', 'PLU_3000181', 'PLU_3000207', 'PLU_3000209',
#        'PLU_3000210', 'PLU_3000211', 'PLU_3000212', 'PLU_3000214',
#        'PLU_3000227', 'PLU_3000277', 'PLU_3000278', 'PLU_3000281',
#        'PLU_3000293', 'PLU_3000296', 'PLU_3000297', 'PLU_6228', 'PLU_810407',
#        'PLU_810703', 'PLU_820221', 'PLU_820224', 'PLU_820602', 'PLU_820801',
#        'PLU_820902', 'PLU_830620', 'PLU_841402', 'PLU_851004','ReceivedQuantity','MissedSales',
#        'CategoryLvl3Desc_Bagels', 'CategoryLvl3Desc_Cakes & Breads',
#        'CategoryLvl3Desc_Cookies & Bars', 'CategoryLvl3Desc_Muffins & Scones',
#        'CategoryLvl3Desc_Pastries', 'CategoryLvl3Desc_Ready to Drink',
#        'DayOfWeek_0', 'DayOfWeek_1',
#        'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5',
#        'DayOfWeek_6', 'Month_1', 'Month_10', 'Month_11', 'Month_12', 'Month_2',
#        'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8',
#        'Month_9', 'Year_2019', 'Year_2020','Temperature']])
cols = ['StoreID_117', 'StoreID_18', 'StoreID_332',
       'PLU_12216', 'PLU_3000024', 'PLU_3000096', 'PLU_3000159', 'PLU_3000162',
       'PLU_3000166', 'PLU_3000181', 'PLU_3000207', 'PLU_3000209',
       'PLU_3000210', 'PLU_3000211', 'PLU_3000212', 'PLU_3000214',
       'PLU_3000227', 'PLU_3000277', 'PLU_3000278', 'PLU_3000281',
       'PLU_3000293', 'PLU_3000296', 'PLU_3000297', 'PLU_6228', 'PLU_810407',
       'PLU_810703', 'PLU_820221', 'PLU_820224', 'PLU_820602', 'PLU_820801',
       'PLU_820902', 'PLU_830620', 'PLU_841402', 'PLU_851004','ReceivedQuantity','MissedSales',
       'CategoryLvl3Desc_Bagels', 'CategoryLvl3Desc_Cakes & Breads',
       'CategoryLvl3Desc_Cookies & Bars', 'CategoryLvl3Desc_Muffins & Scones',
       'CategoryLvl3Desc_Pastries', 'CategoryLvl3Desc_Ready to Drink',
       'DayOfWeek_0', 'DayOfWeek_1',
       'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5',
       'DayOfWeek_6', 'Month_1', 'Month_10', 'Month_11', 'Month_12', 'Month_2',
       'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8',
       'Month_9', 'Year_2019', 'Year_2020','Temperature']
df_18[cols] = scaler.fit_transform(df_18[cols])

# df_18_X = scaler.fit_transform(
#     df_18[['StoreID_117', 'StoreID_18', 'StoreID_332', 'PLU_12216', 'PLU_3000024',
#        'PLU_3000096', 'PLU_3000159', 'PLU_3000162', 'PLU_3000166',
#        'PLU_3000181', 'PLU_3000207', 'PLU_3000209', 'PLU_3000210',
#        'PLU_3000211', 'PLU_3000212', 'PLU_3000214', 'PLU_3000227',
#        'PLU_3000277', 'PLU_3000278', 'PLU_3000281', 'PLU_3000293',
#        'PLU_3000296', 'PLU_3000297', 'PLU_6228', 'PLU_810407', 'PLU_810703',
#        'PLU_820221', 'PLU_820224', 'PLU_820602', 'PLU_820801', 'PLU_820902',
#        'PLU_830620', 'PLU_841402', 'PLU_851004',
#        'ReceivedQuantity', 'MissedSales', 'CategoryLvl3Desc_Bagels',
#        'CategoryLvl3Desc_Cakes & Breads', 'CategoryLvl3Desc_Cookies & Bars',
#        'CategoryLvl3Desc_Muffins & Scones', 'CategoryLvl3Desc_Pastries',
#        'CategoryLvl3Desc_Ready to Drink', 'DayOfWeek_0', 'DayOfWeek_1',
#        'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5',
#        'DayOfWeek_6', 'Month_1', 'Month_10', 'Month_11', 'Month_12', 'Month_2',
#        'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8',
#        'Month_9', 'Year_2019', 'Year_2020']])

# df_18.drop(['BusinessDate','PLU', 'Description', 'ItemType', 'CategoryLvl1Desc','CategoryLvl2Desc','CategoryLvl3Desc','LatestOrder','GroupID'],inplace=True,axis=1)

# df_18 = pd.get_dummies(df_18,columns =['StoreID', 'BusinessDate', 'PLU', 'Description', 'ItemType',
#        'CategoryLvl1Desc', 'CategoryLvl2Desc', 'CategoryLvl3Desc',
#        'ReceivedQuantity', 'SoldQuantity', 'EndQuantity', 'LatestOrder',
#        'StockedOut', 'GroupID', 'MissedSales', 'Temperature'])

# df_18['StockedOut'] = df_18['StockedOut'].astype(float)

# df_18.head()

# df_18['StoreID'] = df_18['StoreID'].astype(str)

# df_18['StoreID'].unique()

# df_18 = pd.get_dummies(df_18,columns =['StoreID'],)

# df_18.columns

# df_18 = df_18[['SoldQuantity','ReceivedQuantity','EndQuantity', 'StockedOut',
#        'MissedSales', 'Temperature','StoreID_117', 'StoreID_18',
#        'StoreID_332']]



"""Regression Models with including Temperature as an input feature"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_18.drop(['SoldQuantity'],axis=1), df_18['SoldQuantity'], test_size=0.2, random_state=3)

# from sklearn.preprocessing import StandardScaler,MinMaxScaler

# scaler = StandardScaler()

# scaler = scaler.fit(X_train)
# X_train = scaler.transform(X_train)

# scaler = scaler.fit(X_test)
# X_test = scaler.transform(X_test)

from sklearn.linear_model import LinearRegression

regression = LinearRegression()
regression.fit(X_train,y_train)
regression.score(X_test,y_test)

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor,XGBRFRegressor

model_params = {
    
    'linear_regression': {
        'model': LinearRegression(),
        'params': {
            #######
        }  
    },
    'ridge_regression': {
        'model': Ridge(),
        'params': {
             'alpha': [0.01,0.1,0.5,1.0]
         }
    },
    'decision_tree': {
        'model': DecisionTreeRegressor(),
        'params': {
             'max_depth':[2,5,10,15,20]
         }  
    },
    'random_forest': {
        'model': RandomForestRegressor(),
        'params': {
             'max_depth':[2,5,10,15,20,25]
         }
    },
        'gbm_regression' : {
        'model': GradientBoostingRegressor(),
        'params' : {
            'learning_rate': [0.2],
            'n_estimators': [100],
             'max_depth' : [5]
         }
    },
        'lgb' : {
        'model': LGBMRegressor(),
        'params' : {
            'learning_rate':[0.01],
            'n_estimators':[1440],
             'max_depth' : [12]
         }
     },
        'XGB_regression' : {
        'model': XGBRegressor(),
        'params' : {
            'learning_rate': [0.1],
            'n_estimators': [1500],
             'max_depth' : [2]
         }
    },
    #     'XGBRF_regression' : {
    #     'model': XGBRFRegressor(),
    #     'params' : {
    #         'learning_rate':[0.5],
    #         'n_estimators':[10,50,100,500,1000,1500],
    #         'max_depth' : [1,2,5,7,10]
    #      }
    # }
}

# Model Selection and hyper-parameter tuning 
from sklearn.model_selection import GridSearchCV

scores = []

for model_name, mp in model_params.items():
    clf =  GridSearchCV(mp['model'],mp['params'], cv=5, return_train_score=False)
    clf.fit(X_train, y_train)
    scores.append({
        'model': model_name,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })
    
df_temp = pd.DataFrame(scores,columns=['model','best_score','best_params'])
df_temp

from joblib import dump, load
dump(clf.best_estimator_, 'reg_mdl_t.joblib')

"""Regression Models without including Temperature as an input feature"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_18.drop(['SoldQuantity','Temperature'],axis=1), df_18['SoldQuantity'], test_size=0.2, random_state=3)


regression = LinearRegression()
regression.fit(X_train,y_train)
regression.score(X_test,y_test)

# Model Selection and hyper-parameter tuning 
from sklearn.model_selection import GridSearchCV

scores = []

for model_name, mp in model_params.items():
    clf =  GridSearchCV(mp['model'],mp['params'], cv=5, return_train_score=False)
    clf.fit(X_train, y_train)
    scores.append({
        'model': model_name,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })
    
df_temp = pd.DataFrame(scores,columns=['model','best_score','best_params'])
df_temp

dump(clf.best_estimator_, 'reg_mdl_nt.joblib')

"""**LSTM Model which takes temperature into consideration**"""

column_indices = {name: i for i, name in enumerate(df_18.columns)}

n = len(df_18)
train_df = df_18[0:int(n*0.7)]
val_df = df_18[int(n*0.7):int(n*0.9)]
test_df = df_18[int(n*0.9):]

num_features = df_18.shape[1]

# train_mean = train_df.mean()
# train_std = train_df.std()

# train_df = (train_df - train_mean) / train_std
# val_df = (val_df - train_mean) / train_std
# test_df = (test_df - train_mean) / train_std

# df_std = (df - train_mean) / train_std
# df_std = df_std.melt(var_name='Column', value_name='Normalized')
# plt.figure(figsize=(12, 6))
# ax = sns.violinplot(x='Column', y='Normalized', data=df_std)
# _ = ax.set_xticklabels(df.keys(), rotation=90)

class WindowGenerator():
  def __init__(self, input_width, label_width, shift,
               train_df=train_df, val_df=val_df, test_df=test_df,
               label_columns=None):
    # Store the raw data.
    self.train_df = train_df
    self.val_df = val_df
    self.test_df = test_df

    # Work out the label column indices.
    self.label_columns = label_columns
    if label_columns is not None:
      self.label_columns_indices = {name: i for i, name in
                                    enumerate(label_columns)}
    self.column_indices = {name: i for i, name in
                           enumerate(train_df.columns)}

    # Work out the window parameters.
    self.input_width = input_width
    self.label_width = label_width
    self.shift = shift

    self.total_window_size = input_width + shift

    self.input_slice = slice(0, input_width)
    self.input_indices = np.arange(self.total_window_size)[self.input_slice]

    self.label_start = self.total_window_size - self.label_width
    self.labels_slice = slice(self.label_start, None)
    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

  def __repr__(self):
    return '\n'.join([
        f'Total window size: {self.total_window_size}',
        f'Input indices: {self.input_indices}',
        f'Label indices: {self.label_indices}',
        f'Label column name(s): {self.label_columns}'])

# w1 = WindowGenerator(input_width=24, label_width=1, shift=1,
#                      label_columns=['SoldQuantity'])
# w2 = WindowGenerator(input_width=24, label_width=1, shift=3,
#                      label_columns=['SoldQuantity'])
# w3 = WindowGenerator(input_width=24, label_width=1, shift=10,
#                      label_columns=['SoldQuantity'])
# w1

# w2 = WindowGenerator(input_width=6, label_width=1, shift=1,
#                      label_columns=['SoldQuantity'])
# w2

def split_window(self, features):
  inputs = features[:, self.input_slice, :]
  labels = features[:, self.labels_slice, :]
  if self.label_columns is not None:
    labels = tf.stack(
        [labels[:, :, self.column_indices[name]] for name in self.label_columns],
        axis=-1)

  # Slicing doesn't preserve static shape information, so set the shapes
  # manually. This way the `tf.data.Datasets` are easier to inspect.
  inputs.set_shape([None, self.input_width, None])
  labels.set_shape([None, self.label_width, None])

  return inputs, labels

WindowGenerator.split_window = split_window

# example_window = tf.stack([np.array(train_df[:w2.total_window_size]),
#                            np.array(train_df[100:100+w2.total_window_size]),
#                            np.array(train_df[200:200+w2.total_window_size])])

# example_inputs, example_labels = w2.split_window(example_window)

# print('All shapes are: (batch, time, features)')
# print(f'Window shape: {example_window.shape}')
# print(f'Inputs shape: {example_inputs.shape}')
# print(f'Labels shape: {example_labels.shape}')

# w2.example = example_inputs, example_labels

def plot(self, model=None, plot_col='SoldQuantity', max_subplots=3):
  inputs, labels = self.example
  plt.figure(figsize=(12, 8))
  plot_col_index = self.column_indices[plot_col]
  max_n = min(max_subplots, len(inputs))
  for n in range(max_n):
    plt.subplot(max_n, 1, n+1)
    plt.ylabel(f'{plot_col} [normed]')
    plt.plot(self.input_indices, inputs[n, :, plot_col_index],
             label='Inputs', marker='.', zorder=-10)

    if self.label_columns:
      label_col_index = self.label_columns_indices.get(plot_col, None)
    else:
      label_col_index = plot_col_index

    if label_col_index is None:
      continue

    plt.scatter(self.label_indices, labels[n, :, label_col_index],
                edgecolors='k', label='Labels', c='#2ca02c', s=64)
    if model is not None:
      predictions = model(inputs)
      plt.scatter(self.label_indices, predictions[n, :, label_col_index],
                  marker='X', edgecolors='k', label='Predictions',
                  c='#ff7f0e', s=64)

    if n == 0:
      plt.legend()

  plt.xlabel('days')

WindowGenerator.plot = plot

# w2.plot()

def make_dataset(self, data):
  data = np.array(data, dtype=np.float32)
  ds = tf.keras.utils.timeseries_dataset_from_array(
      data=data,
      targets=None,
      sequence_length=self.total_window_size,
      sequence_stride=1,
      shuffle=True,
      batch_size=512,)

  ds = ds.map(self.split_window)

  return ds

WindowGenerator.make_dataset = make_dataset

@property
def train(self):
  return self.make_dataset(self.train_df)

@property
def val(self):
  return self.make_dataset(self.val_df)

@property
def test(self):
  return self.make_dataset(self.test_df)

@property
def example(self):
  """Get and cache an example batch of `inputs, labels` for plotting."""
  result = getattr(self, '_example', None)
  if result is None:
    # No example batch was found, so get one from the `.train` dataset
    result = next(iter(self.train))
    # And cache it for next time
    self._example = result
  return result

WindowGenerator.train = train
WindowGenerator.val = val
WindowGenerator.test = test
WindowGenerator.example = example

# # # Each element is an (inputs, label) pair.
# w2.train.element_spec

# for example_inputs, example_labels in w2.train.take(1):
#   print(f'Inputs shape (batch, time, features): {example_inputs.shape}')
#   print(f'Labels shape (batch, time, features): {example_labels.shape}')

# single_step_window = WindowGenerator(
#     input_width=1, label_width=1, shift=1,
#     label_columns=['SoldQuantity'])
# single_step_window

# for example_inputs, example_labels in single_step_window.train.take(1):
#   print(f'Inputs shape (batch, time, features): {example_inputs.shape}')
#   print(f'Labels shape (batch, time, features): {example_labels.shape}')

# class Baseline(tf.keras.Model):
#   def __init__(self, label_index=None):
#     super().__init__()
#     self.label_index = label_index

#   def call(self, inputs):
#     if self.label_index is None:
#       return inputs
#     result = inputs[:, :, self.label_index]
#     return result[:, :, tf.newaxis]

# baseline = Baseline(label_index=column_indices['SoldQuantity'])

# baseline.compile(loss=tf.losses.MeanSquaredError(),
#                  metrics=[tf.metrics.MeanAbsoluteError()])

# val_performance = {}
# performance = {}
# val_performance['Baseline'] = baseline.evaluate(single_step_window.val)
# performance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)

# wide_window = WindowGenerator(
#     input_width=24, label_width=24, shift=1,
#     label_columns=['SoldQuantity'])

# wide_window

# print('Input shape:', wide_window.example[0].shape)
# print('Output shape:', baseline(wide_window.example[0]).shape)

# wide_window.plot(baseline)

# linear = tf.keras.Sequential([
#     tf.keras.layers.Dense(units=1)
# ])

# print('Input shape:', single_step_window.example[0].shape)
# print('Output shape:', linear(single_step_window.example[0]).shape)

# MAX_EPOCHS = 20

# def compile_and_fit(model, window, patience=2):
#   # early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
#   #                                                   patience=patience,
#   #                                                   mode='min')

#   model.compile(loss=tf.losses.MeanSquaredError(),
#                 optimizer=tf.optimizers.Adam(learning_rate=0.0005),
#                 metrics=[tf.metrics.MeanAbsoluteError()])

#   history = model.fit(window.train, epochs=MAX_EPOCHS,
#                       validation_data=window.val)
#   # return history

# history = compile_and_fit(linear, single_step_window)

# val_performance['Linear'] = linear.evaluate(single_step_window.val)
# performance['Linear'] = linear.evaluate(single_step_window.test, verbose=2)

# print('Input shape:', wide_window.example[0].shape)
# print('Output shape:', baseline(wide_window.example[0]).shape)

# wide_window.plot(linear)

# plt.bar(x = range(len(train_df.columns)),
#         height=linear.layers[0].kernel[:,0].numpy())
# axis = plt.gca()
# axis.set_xticks(range(len(train_df.columns)))
# _ = axis.set_xticklabels(train_df.columns, rotation=90)

# dense = tf.keras.Sequential([
#     tf.keras.layers.Dense(units=64, activation='relu'),
#     tf.keras.layers.Dense(units=64, activation='relu'),
#     tf.keras.layers.Dense(units=1)
# ])

# history = compile_and_fit(dense, single_step_window)

# val_performance['Dense'] = dense.evaluate(single_step_window.val)
# performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)

# CONV_WIDTH = 3
# conv_window = WindowGenerator(
#     input_width=CONV_WIDTH,
#     label_width=1,
#     shift=1,
#     label_columns=['SoldQuantity'])

# conv_window

# conv_window.plot()
# plt.title("Given 3 hours of inputs, predict 1 hour into the future.")

# multi_step_dense = tf.keras.Sequential([
#     # Shape: (time, features) => (time*features)
#     tf.keras.layers.Flatten(),
#     tf.keras.layers.Dense(units=32, activation='relu'),
#     tf.keras.layers.Dense(units=32, activation='relu'),
#     tf.keras.layers.Dense(units=1),
#     # Add back the time dimension.
#     # Shape: (outputs) => (1, outputs)
#     tf.keras.layers.Reshape([1, -1]),
# ])

# print('Input shape:', conv_window.example[0].shape)
# print('Output shape:', multi_step_dense(conv_window.example[0]).shape)

# history = compile_and_fit(multi_step_dense, conv_window)

# IPython.display.clear_output()
# val_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.val)
# performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.test, verbose=5)

# conv_window.plot(multi_step_dense)

# conv_model = tf.keras.Sequential([
#     tf.keras.layers.Conv1D(filters=32,
#                            kernel_size=(CONV_WIDTH,),
#                            activation='relu'),
#     tf.keras.layers.Dense(units=32, activation='relu'),
#     tf.keras.layers.Dense(units=1),
# ])

# print("Conv model on `conv_window`")
# print('Input shape:', conv_window.example[0].shape)
# print('Output shape:', conv_model(conv_window.example[0]).shape)

# history = compile_and_fit(conv_model, conv_window)

# IPython.display.clear_output()
# val_performance['Conv'] = conv_model.evaluate(conv_window.val)
# performance['Conv'] = conv_model.evaluate(conv_window.test, verbose=0)

# print("Wide window")
# print('Input shape:', wide_window.example[0].shape)
# print('Labels shape:', wide_window.example[1].shape)
# print('Output shape:', conv_model(wide_window.example[0]).shape)

# LABEL_WIDTH = 24
# INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)
# wide_conv_window = WindowGenerator(
#     input_width=INPUT_WIDTH,
#     label_width=LABEL_WIDTH,
#     shift=1,
#     label_columns=['SoldQuantity'])

# wide_conv_window

# print("Wide conv window")
# print('Input shape:', wide_conv_window.example[0].shape)
# print('Labels shape:', wide_conv_window.example[1].shape)
# print('Output shape:', conv_model(wide_conv_window.example[0]).shape)

# wide_conv_window.plot(conv_model)

# lstm_model = tf.keras.models.Sequential([
#     # Shape [batch, time, features] => [batch, time, lstm_units]
#     tf.keras.layers.LSTM(32, return_sequences=True),
#     tf.keras.layers.LSTM(32, return_sequences=True),
#     # Shape => [batch, time, features]
#     tf.keras.layers.Dense(units=1)
# ])

# print('Input shape:', wide_window.example[0].shape)
# print('Output shape:', lstm_model(wide_window.example[0]).shape)

# history = compile_and_fit(lstm_model, wide_window)

# IPython.display.clear_output()
# val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)
# performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)

# wide_window.plot(lstm_model)

# x = np.arange(len(performance))
# width = 0.3
# metric_name = 'mean_absolute_error'
# metric_index = lstm_model.metrics_names.index('mean_absolute_error')
# val_mae = [v[metric_index] for v in val_performance.values()]
# test_mae = [v[metric_index] for v in performance.values()]

# plt.ylabel('mean_absolute_error [SlodQuantity, normalized]')
# plt.bar(x - 0.17, val_mae, width, label='Validation')
# plt.bar(x + 0.17, test_mae, width, label='Test')
# plt.xticks(ticks=x, labels=performance.keys(),
#            rotation=45)
# _ = plt.legend()

wide_window_1 = WindowGenerator(
    input_width=24, label_width=24, shift=1,
    label_columns=['SoldQuantity'])

wide_window_2 = WindowGenerator(
    input_width=24, label_width=24, shift=3,
    label_columns=['SoldQuantity'])

wide_window_3 = WindowGenerator(
    input_width=24, label_width=24, shift=10,
    label_columns=['SoldQuantity'])

wide_window_1

# print('Input shape:', wide_window_1.example[0].shape)
# print('Output shape:', baseline(wide_window_1.example[0]).shape)

from gc import callbacks
MAX_EPOCHS = 10

def compile_and_fit(model, window, patience=4):
  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                    patience=patience,
                                                    mode='min')

  model.compile(loss=tf.losses.MeanSquaredError(),
                optimizer=tf.optimizers.Adam(learning_rate=0.0002),
                metrics=[tf.metrics.MeanAbsoluteError()])

  history = model.fit(window.train, epochs=MAX_EPOCHS,
                      validation_data=window.val)
  return history

lstm_model = tf.keras.models.Sequential([
    # Shape [batch, time, features] => [batch, time, lstm_units]
    tf.keras.layers.LSTM(64,activation='relu', return_sequences=True),
    tf.keras.layers.LSTM(32,activation='relu', return_sequences=True),
    # tf.keras.layers.LSTM(32,activation='relu', return_sequences=False),
    # tf.keras.layers.LSTM(32,activation='relu', return_sequences=True),
    # Shape => [batch, time, features]
    tf.keras.layers.Dense(units=32),
    tf.keras.layers.Dense(units=16),
    tf.keras.layers.Dense(units=1)
])

print('Input shape:', wide_window_1.example[0].shape)
print('Output shape:', lstm_model(wide_window_1.example[0]).shape)

history_1 = compile_and_fit(lstm_model, wide_window_1)

lstm_model.save('lstm_1')

wide_window_1.plot(lstm_model)

history_2 = compile_and_fit(lstm_model, wide_window_2)

wide_window_2.plot(lstm_model)

history_3 = compile_and_fit(lstm_model, wide_window_3)

wide_window_3.plot(lstm_model)

lstm_model.save("lstm_10")

"""# Section 3:  Inventory optimization"""

df.head()
df_ip = df[((((df['StoreID']==18) & (df['PLU']==3000227) )))].copy()
df_ip['DayOfMonth'] = df_ip['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day).astype(str)
df_ip['DayOfWeek'] = df_ip['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).day_of_week).astype(str)
df_ip['Month'] = df_ip['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).month).astype(str)
df_ip['Year'] = df_ip['BusinessDate'].apply(lambda x:(pd.Timestamp(x)).year).astype(str)

df_ip['BusinessDate'] = pd.to_datetime(df['BusinessDate'])
df_ip['week'] = df_ip['BusinessDate'].dt.week

df_ip = df_ip.drop(['Description','ItemType','CategoryLvl1Desc','CategoryLvl2Desc'],axis=1)

df_ip_month_5 = df_ip[(df_ip['Month'].astype('int')==12)].copy()

df_ip_month_5['Revenue'] = df_ip_month_5['SoldQuantity'] * 2.5
df_temp = df_ip_month_5.groupby(['PLU','DayOfWeek']).sum(['Revenue'])

import seaborn as sns
sns.distplot(df_temp['Revenue'])

def bins(x):
    for bar in range(10, 200, 10):
        if x <= bar:
            return bar
# Create new column to apply the bin function
df_temp["Rev_dist"] = df_temp["Revenue"].apply(lambda x: bins(x))

df_temp["count"] = 1
# Create a pivot table of the revenue distributions
pivot_table = pd.pivot_table(df_temp, index = ["Rev_dist"], values = ["count"], aggfunc = np.sum)

from sklearn.cluster import KMeans
# K -clusters is equal to 3 because things will be sorted into A, B, and C
kmeans = KMeans(n_clusters=3)
kmeans.fit(pivot_table)

pivot_table["category"] = kmeans.labels_

ABC_dict = {    
    0: "A",
    1: "C",
    2: "B",
}
pivot_table["ABC"] = pivot_table["category"].apply(lambda x: ABC_dict[x])

df_temp = pd.merge(df_temp, pivot_table, on = "Rev_dist", how ="left")

df_temp.head()

df_Cat_A = df_temp[df_temp['ABC'] == 'A']

df_Cat_A.count()

df_Cat_A.head()

"""## **Initiatives**"""

df_inititaives = df_ip[((df_ip['Month'].astype(int)>=7) & (df_ip['Month'].astype(int)<=12) )].copy()

df_inititaives.info()

def calc_mean(date,week,main_data):
    date = date.strftime('%Y-%m-%d')
    main_data_ = main_data[main_data['DayOfWeek'].astype('str')==week]
    main_data_['BusinessDate'] = main_data_['BusinessDate'].dt.strftime('%Y-%m-%d')
    main_data_ = main_data_.drop(['GroupID','MissedSales','StoreID','LatestOrder','PLU'],axis=1)
    sub_df = main_data_[((main_data_['BusinessDate']<date))]
    a_df = sub_df[-4:]
    a = a_df['SoldQuantity'].mean()
    return a
#     for index, row in sub_df.iterrows():
#         print(row[0], row[1])

df_inititaives.columns

df_ = df_inititaives.copy()
df_inititaives['mean_sq_prev_4days'] = df_inititaives.apply(lambda x: calc_mean(x['BusinessDate'],x['DayOfWeek'],df_),axis=1)
#df_mod

df_inititaives

df_mod = df_inititaives.copy()

df_mod.loc[:,'mean_sq_prev_4days'] = df_mod.mean_sq_prev_4days.shift(-2)

df_mod.dropna()

df_mod.loc[:,'shifted_EQ'] = df_mod.EndQuantity.shift(-1)

RQ_avg = df_mod['ReceivedQuantity'].mean()

df_mod['New_RecievedQuantity'] = df_mod['mean_sq_prev_4days']+df_mod['shifted_EQ']+RQ_avg

df_mod.describe()

df_mod = df_mod.drop(['mean_sq_prev_4days'],axis=1)

df_mod.dropna()

df_upd = df_mod.copy()
df_upd.drop(['BusinessDate'],axis=1,inplace=True)

df_upd.columns

df_upd['StoreID'] = df_upd['StoreID'].astype(str)
df_upd['PLU'] = df_upd['PLU'].astype(str)
df_upd['DayOfWeek'] = df_upd['DayOfWeek'].astype(str)
df_upd['Month'] = df_upd['Month'].astype(str)
df_upd['Year'] = df_upd['Year'].astype(str)

df_upd.drop(['DayOfMonth','GroupID','ReceivedQuantity','LatestOrder','StockedOut','GroupID','week','shifted_EQ','CategoryLvl3Desc'],axis=1,inplace=True)

df_upd = pd.get_dummies(df_upd,['StoreID', 'PLU','DayOfWeek','Month', 'Year'])

df_upd.columns

df_inititaives.columns

nw_cols = ['StoreID_117', 'StoreID_332',
       'PLU_12216', 'PLU_3000024', 'PLU_3000096', 'PLU_3000159', 'PLU_3000162',
       'PLU_3000166', 'PLU_3000181', 'PLU_3000207', 'PLU_3000209',
       'PLU_3000210', 'PLU_3000211', 'PLU_3000212', 'PLU_3000214',
        'PLU_3000277', 'PLU_3000278', 'PLU_3000281',
       'PLU_3000293', 'PLU_3000296', 'PLU_3000297', 'PLU_6228', 'PLU_810407',
       'PLU_810703', 'PLU_820221', 'PLU_820224', 'PLU_820602', 'PLU_820801',
       'PLU_820902', 'PLU_830620', 'PLU_841402', 'PLU_851004','SoldQuantity',
       'CategoryLvl3Desc_Bagels', 'CategoryLvl3Desc_Cakes & Breads',
       'CategoryLvl3Desc_Cookies & Bars', 'CategoryLvl3Desc_Muffins & Scones',
       'CategoryLvl3Desc_Pastries', 'CategoryLvl3Desc_Ready to Drink','Month_1', 'Month_2',
       'Month_3', 'Month_4', 'Month_5', 'Month_6','Year_2020',]

df_upd[nw_cols] = 0

df_upd = df_upd.rename(columns={'New_RecievedQuantity': 'ReceivedQuantity'})

cols = ['ReceivedQuantity','MissedSales','Temperature']

df_upd[cols] = scaler.fit_transform(df_upd[cols])

df_upd = df_upd[['StoreID_117', 'StoreID_18', 'StoreID_332', 'PLU_12216', 'PLU_3000024',
       'PLU_3000096', 'PLU_3000159', 'PLU_3000162', 'PLU_3000166',
       'PLU_3000181', 'PLU_3000207', 'PLU_3000209', 'PLU_3000210',
       'PLU_3000211', 'PLU_3000212', 'PLU_3000214', 'PLU_3000227',
       'PLU_3000277', 'PLU_3000278', 'PLU_3000281', 'PLU_3000293',
       'PLU_3000296', 'PLU_3000297', 'PLU_6228', 'PLU_810407', 'PLU_810703',
       'PLU_820221', 'PLU_820224', 'PLU_820602', 'PLU_820801', 'PLU_820902',
       'PLU_830620', 'PLU_841402', 'PLU_851004', 'SoldQuantity',
       'ReceivedQuantity', 'MissedSales', 'CategoryLvl3Desc_Bagels',
       'CategoryLvl3Desc_Cakes & Breads', 'CategoryLvl3Desc_Cookies & Bars',
       'CategoryLvl3Desc_Muffins & Scones', 'CategoryLvl3Desc_Pastries',
       'CategoryLvl3Desc_Ready to Drink', 'DayOfWeek_0', 'DayOfWeek_1',
       'DayOfWeek_2', 'DayOfWeek_3', 'DayOfWeek_4', 'DayOfWeek_5',
       'DayOfWeek_6', 'Month_1', 'Month_10', 'Month_11', 'Month_12', 'Month_2',
       'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7', 'Month_8',
       'Month_9', 'Year_2019', 'Year_2020', 'Temperature']]

df_18.columns

df.describe()

df_upd.head()



model = tf.keras.models.load_model('lstm_1')

class WindowGenerator():
  def __init__(self, input_width, label_width, shift,
               train_df=train_df, val_df=val_df, test_df=df_upd,
               label_columns=None):
    # Store the raw data.
    self.train_df = train_df
    self.val_df = val_df
    self.df_upd = df_upd

    # Work out the label column indices.
    self.label_columns = label_columns
    if label_columns is not None:
      self.label_columns_indices = {name: i for i, name in
                                    enumerate(label_columns)}
    self.column_indices = {name: i for i, name in
                           enumerate(train_df.columns)}

    # Work out the window parameters.
    self.input_width = input_width
    self.label_width = label_width
    self.shift = shift

    self.total_window_size = input_width + shift

    self.input_slice = slice(0, input_width)
    self.input_indices = np.arange(self.total_window_size)[self.input_slice]

    self.label_start = self.total_window_size - self.label_width
    self.labels_slice = slice(self.label_start, None)
    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

  def __repr__(self):
    return '\n'.join([
        f'Total window size: {self.total_window_size}',
        f'Input indices: {self.input_indices}',
        f'Label indices: {self.label_indices}',
        f'Label column name(s): {self.label_columns}'])

WindowGenerator.split_window = split_window
WindowGenerator.make_dataset = make_dataset


@property
def test(self):
  return self.make_dataset(self.df_upd)

@property
def example(self):
  """Get and cache an example batch of `inputs, labels` for plotting."""
  result = getattr(self, '_example', None)
  if result is None:
    # No example batch was found, so get one from the `.train` dataset
    result = next(iter(self.train))
    # And cache it for next time
    self._example = result
  return result

WindowGenerator.test = test

wide_window_1 = WindowGenerator(
    input_width=24, label_width=24, shift=1,
    label_columns=['SoldQuantity'])

wide_window_1.test

model.predict(wide_window_1.test).reshape(160,24)[0]

scaler.inverse_transform(df_upd[cols]).shape

df_upd.head()

loaded_model = load("reg_mdl_t.joblib")

sales_sum = np.sum(loaded_model.predict(df_upd.drop(['SoldQuantity'],axis=1)))

print(sales_sum)
total_sales = sales_sum*3

df_mod['SoldQuantity'].sum() *3

total_sales



"""Dashboard"""

!pip install explainerdashboard

from explainerdashboard import ClassifierExplainer, ExplainerDashboard,RegressionExplainer

explainer = RegressionExplainer(clf.best_estimator_, X_test, y_test)

ExplainerDashboard(explainer).run()

